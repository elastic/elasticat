# EDOT (Elastic Distribution of OpenTelemetry) Configuration
# Optimized for local development log collection

exporters:
  # Debug exporter for troubleshooting
  debug:
    verbosity: basic

  # Elasticsearch exporter with OTel mapping
  elasticsearch/otel:
    endpoints:
      - ${env:ELASTICSEARCH_ENDPOINT}
    logs_dynamic_index:
      enabled: true
    metrics_dynamic_index:
      enabled: true
    traces_dynamic_index:
      enabled: true
    mapping:
      mode: otel
    flush:
      bytes: 5242880  # 5MB
      interval: 5s
    retry:
      enabled: true
      initial_interval: 100ms
      max_interval: 30s

receivers:
  # OTLP receiver for instrumented applications
  otlp:
    protocols:
      grpc:
        endpoint: ${env:OTEL_COLLECTOR_HOST}:${env:OTEL_COLLECTOR_PORT_GRPC}
      http:
        endpoint: ${env:OTEL_COLLECTOR_HOST}:${env:OTEL_COLLECTOR_PORT_HTTP}
        cors:
          allowed_origins:
            - "http://*"
            - "https://*"

  # Docker container logs (disabled by default - Docker-specific, not Podman compatible)
  # To enable: uncomment this receiver, the logs/docker pipeline, and the Docker volume mounts
  # filelog/docker:
  #   include:
  #     - /var/lib/docker/containers/*/*.log
  #   start_at: end
  #   include_file_path: true
  #   include_file_name: false
  #   operators:
  #     - type: json_parser
  #       id: parser-docker
  #       timestamp:
  #         parse_from: attributes.time
  #         layout: '%Y-%m-%dT%H:%M:%S.%fZ'
  #     - type: regex_parser
  #       id: extract_container_id
  #       regex: '^.*containers/(?P<container_id>[^/]+)/.*\.log$'
  #       parse_from: attributes["log.file.path"]
  #     - type: move
  #       id: move_log
  #       from: attributes.log
  #       to: body

  # Local application log files
  # Watches /var/log/app which is mounted from host's ./logs directory
  # Supports: *.log files (plain text or JSON)
  filelog/app:
    include:
      - /var/log/app/**/*.log
      - /var/log/app/*.log
    exclude:
      - /var/log/app/**/*.gz
      - /var/log/app/**/*.zip
    start_at: end
    include_file_path: true
    include_file_name: true
    poll_interval: 500ms
    operators:
      # Try to parse as JSON first, fall back to plain text
      - type: router
        id: check_json
        routes:
          - output: json_parser
            expr: 'body matches "^\\s*\\{"'
        default: plain_text

      # JSON log parsing
      - type: json_parser
        id: json_parser
        output: set_service_json
        on_error: send  # If JSON parsing fails, send as-is

      # Extract service name from filename for JSON logs
      - type: regex_parser
        id: set_service_json
        regex: '(?P<service_name>[^/]+?)(?:-(?P<log_type>err|error|out|info|debug))?\.log$'
        parse_from: attributes["log.file.name"]
        output: merge_json
        on_error: send

      # Merge common JSON fields
      - type: move
        id: merge_json
        from: attributes.message
        to: body
        on_error: send

      # Plain text log parsing
      - type: regex_parser
        id: plain_text
        regex: '(?P<service_name>[^/]+?)(?:-(?P<log_type>err|error|out|info|debug))?\.log$'
        parse_from: attributes["log.file.name"]
        output: detect_level
        on_error: send

      # Try to detect log level from common patterns
      - type: regex_parser
        id: detect_level
        regex: '(?i)^(?P<timestamp>\d{4}[-/]\d{2}[-/]\d{2}[T ]\d{2}:\d{2}:\d{2}[.,]?\d*\s*(?:Z|[+-]\d{2}:?\d{2})?)\s*(?:\[?\s*(?P<level>TRACE|DEBUG|INFO|WARN(?:ING)?|ERROR|FATAL|CRITICAL)\s*\]?)?'
        parse_from: body
        on_error: send

      # Set severity from detected level
      - type: severity_parser
        id: set_severity
        parse_from: attributes.level
        preset: none
        mapping:
          trace: [TRACE, trace]
          debug: [DEBUG, debug]
          info: [INFO, info]
          warn: [WARN, WARNING, warn, warning]
          error: [ERROR, error, ERR, err]
          fatal: [FATAL, fatal, CRITICAL, critical]
        on_error: send

  # Host metrics for system observability (disabled by default - requires /hostfs mount)
  # To enable: uncomment this receiver, the metrics/host pipeline, and the hostfs volume mount
  # hostmetrics:
  #   collection_interval: 30s
  #   root_path: /hostfs
  #   scrapers:
  #     cpu:
  #       metrics:
  #         system.cpu.utilization:
  #           enabled: true
  #     memory:
  #       metrics:
  #         system.memory.utilization:
  #           enabled: true
  #     disk:
  #     filesystem:
  #       exclude_mount_points:
  #         mount_points:
  #           - /dev/*
  #           - /proc/*
  #           - /sys/*
  #           - /run/*
  #           - /var/lib/docker/*
  #         match_type: regexp
  #     network:
  #     load:

processors:
  # Batch for efficient sending
  batch:
    timeout: 5s
    send_batch_size: 256

  batch/metrics:
    send_batch_max_size: 0
    timeout: 1s

  # Elastic-specific processors
  elasticinframetrics:
    add_system_metrics: true
    add_k8s_metrics: false
    drop_original: true

  elastictrace: {}

  # Resource detection
  resourcedetection/system:
    detectors: ["system"]
    system:
      hostname_sources: ["os"]
      resource_attributes:
        host.name:
          enabled: true
        host.arch:
          enabled: true
        os.type:
          enabled: true

  # Add turbodevlog resource attributes
  resource:
    attributes:
      - key: deployment.environment
        value: development
        action: upsert
      - key: service.namespace
        value: turbodevlog
        action: upsert

  # Transform for Docker log processing
  transform/docker:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(resource.attributes["service.name"], "docker") where resource.attributes["service.name"] == nil

  # Transform for app log processing - set service name from filename
  transform/app:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Set service.name from the parsed service_name attribute
          - set(resource.attributes["service.name"], attributes["service_name"]) where attributes["service_name"] != nil
          # Set log type (err, out, etc.) as an attribute
          - set(attributes["log.type"], attributes["log_type"]) where attributes["log_type"] != nil
          # Mark as file-based log
          - set(attributes["log.source"], "file")

connectors:
  # Generate span metrics
  spanmetrics:

  # Elastic APM connector
  elasticapm:

service:
  pipelines:
    # Logs from OTLP (instrumented apps)
    logs/otlp:
      receivers: [otlp]
      processors: [resource, batch]
      exporters: [elasticsearch/otel, debug]

    # Logs from Docker containers (disabled - Docker-specific)
    # logs/docker:
    #   receivers: [filelog/docker]
    #   processors: [transform/docker, resource, batch]
    #   exporters: [elasticsearch/otel]

    # Logs from local application files (./logs/*.log)
    logs/app:
      receivers: [filelog/app]
      processors: [transform/app, resource, batch]
      exporters: [elasticsearch/otel, debug]

    # Metrics from OTLP
    metrics:
      receivers: [otlp, spanmetrics]
      processors: [batch/metrics]
      exporters: [elasticsearch/otel, elasticapm]

    # Host metrics (disabled - requires /hostfs mount)
    # metrics/host:
    #   receivers: [hostmetrics]
    #   processors: [elasticinframetrics, resourcedetection/system, batch/metrics]
    #   exporters: [elasticsearch/otel]

    # Traces
    traces:
      receivers: [otlp]
      processors: [elastictrace, batch]
      exporters: [elasticsearch/otel, spanmetrics, elasticapm]

    # Aggregated APM metrics
    metrics/apm:
      receivers: [elasticapm]
      processors: []
      exporters: [elasticsearch/otel]

  telemetry:
    logs:
      level: info
